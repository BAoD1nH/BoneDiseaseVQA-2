import os
import torch
import json
from transformers import AutoTokenizer, AutoFeatureExtractor
from model_arch import BoneDiseaseVQA
from dataset import MedicalVQADataset

# Constants
VISION_MODEL = 'microsoft/swinv2-base-patch4-window8-256'
TEXT_MODEL = 'vimednli/vihealthbert-w_mlm-ViMedNLI'
MODEL_PATH = 'checkpoints/best_model.pt'
LABEL_MAP_FILE = 'label_map_idx2label.json'
IMAGE_ROOT = '/kaggle/input/bonevqa/DemoBoneData'  # Adjust as necessary

# Load label mapping
with open(LABEL_MAP_FILE, 'r', encoding='utf-8') as f:
    idx2label = json.load(f)

# Load model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = BoneDiseaseVQA(
    vision_model_name=VISION_MODEL,
    text_model_name=TEXT_MODEL,
    answer_classes=list(idx2label.values())
).to(device)
model.load_state_dict(torch.load(MODEL_PATH))
model.eval()

# Load tokenizer and feature extractor
tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)
feature_extractor = AutoFeatureExtractor.from_pretrained(VISION_MODEL)

def infer(image_path, question):
    # Prepare inputs
    pixel_values = feature_extractor(images=image_path, return_tensors="pt").pixel_values.to(device)
    inputs = tokenizer(question, return_tensors="pt", padding=True, truncation=True).to(device)

    with torch.no_grad():
        logits = model(pixel_values, inputs['input_ids'], inputs['attention_mask'])
        preds = torch.argmax(logits, dim=1)
    
    return idx2label[str(preds.item())]

# Example usage
# result = infer('path_to_image.jpg', 'What is the diagnosis?')
# print(result)